import os
import logging
import requests
import google.generativeai as genai
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from qdrant_client.http.models import Filter, FieldCondition, MatchAny

import json

logging.basicConfig(level=logging.INFO)
load_dotenv("key.env")

QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "documents")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    logging.error("‚ùå Missing GEMINI_API_KEY in environment variables")
    raise SystemExit("Missing GEMINI_API_KEY")

genai.configure(api_key=GEMINI_API_KEY)
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
app = FastAPI()

def get_embedding(text):
    try:
        url = "http://192.168.1.10:11434/api/embeddings"
        payload = {
            "model": "hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:latest",
            "prompt": text
        }
        response = requests.post(url, json=payload)
        response.raise_for_status()
        data = response.json()
        if 'embedding' in data and isinstance(data['embedding'], list):
            embedding = data['embedding']
            logging.info(f"‚úÖ API OK: embedding length {len(embedding)}")
            return embedding
        else:
            logging.error(f"‚ùå API Response ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {data}")
            return None
    except Exception as e:
        logging.error(f"‚ùå Embedding error: {e}")
        return None

def extract_metadata_filter_with_llm(query: str):
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó metadata filter ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢ key ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå metadata ‡πÅ‡∏•‡∏∞ value ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á

‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö metadata ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ß‡πà‡∏≤‡∏á: {{}}
"""
    model = genai.GenerativeModel('gemini-1.5-pro')
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": 128,
                "temperature": 0,
            }
        )
        data = json.loads(response.text.strip())
        if isinstance(data, dict) and data:
            return data
    except Exception as e:
        logging.error(f"‚ùå LLM extract metadata filter error: {e}")
    return None

def build_metadata_filter(metadata_dict):
    if not metadata_dict:
        return None

    must_conditions = []
    for key, value in metadata_dict.items():
        if value:
            # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ value ‡πÄ‡∏õ‡πá‡∏ô string ‡∏´‡∏£‡∏∑‡∏≠ number ‡∏ï‡∏£‡∏á‡πÜ
            must_conditions.append(
                FieldCondition(
                    key=key,
                    match=MatchAny(any=[str(value)])
                )
            )
    if not must_conditions:
        return None

    return Filter(must=must_conditions)

def search_documents(query_vector, collection, limit=40, exclude_ids=None, metadata_filter=None):
    try:
        filter_condition = metadata_filter

        if exclude_ids:
            if filter_condition is None:
                filter_condition = Filter(
                    must_not=[
                        FieldCondition(
                            key="‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ",
                            match=MatchAny(any=[str(eid) for eid in exclude_ids])
                        )
                    ]
                )
            else:
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ filter ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≠ must_not ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
                filter_condition.must_not = filter_condition.must_not or []
                filter_condition.must_not.append(
                    FieldCondition(
                        key="‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ",
                        match=MatchAny(any=[str(eid) for eid in exclude_ids])
                    )
                )

        results = qdrant_client.search(
            collection_name=collection,
            query_vector=query_vector,
            limit=limit,
            with_payload=True,
            query_filter=filter_condition
        )
        return results
    except Exception as e:
        logging.error(f"‚ùå Qdrant search error: {e}")
        return []

def format_context(hit):
    account_id = hit.payload.get('‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '')
    account_name = hit.payload.get('‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '')
    account_type = hit.payload.get('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '')
    description = hit.payload.get('‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '')
    return (
        f"‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ: {account_id}, "
        f"‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ: {account_name}, "
        f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ: {account_type}, "
        f"‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ: {description}"
    )

def llm_fn(query, contexts):
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î):
{chr(10).join(contexts)}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô' ‡∏´‡∏£‡∏∑‡∏≠ '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ'

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô:
"""
    model = genai.GenerativeModel('gemini-1.5-pro')
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": 512,
                "temperature": 0.5,
            }
        )
        return response.text.strip()
    except Exception as e:
        logging.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å Gemini API (SDK): {e}")
        return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"

def rerank_by_relevance(query, contexts):
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡πÇ‡∏õ‡∏£‡∏î‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏ß‡πà‡∏≤‡∏°‡∏µ "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á" ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà:
{chr(10).join(f"{i+1}. {ctx}" for i, ctx in enumerate(contexts))}

‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£:
- ‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
- ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢, ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç, ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
- ‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÑ‡∏ß‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô: 2,1,4,3,5

‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

‡∏ñ‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏¢:
"""
    model = genai.GenerativeModel("gemini-1.5-pro")
    try:
        response = model.generate_content(prompt)
        order = [int(i.strip()) - 1 for i in response.text.strip().split(",") if i.strip().isdigit()]
        return [contexts[i] for i in order if i < len(contexts)]
    except Exception as e:
        logging.error(f"‚ùå Rerank error: {e}")
        return contexts

def process_search_and_answer(query, collection, top_k=40, exclude_ids=None):
    query_vector = get_embedding(query)
    if not query_vector:
        return None, []

    metadata_dict = extract_metadata_filter_with_llm(query)
    metadata_filter = build_metadata_filter(metadata_dict)

    results = search_documents(query_vector, collection, limit=top_k, exclude_ids=exclude_ids, metadata_filter=metadata_filter)

    if not results:
        return None, []

    context_with_scores = [
        (format_context(hit), hit.score if hasattr(hit, "score") else 0.0)
        for hit in results
    ]

    context_with_scores.sort(key=lambda x: x[1], reverse=True)
    top_contexts_for_rerank = [ctx for ctx, _ in context_with_scores[:15]]

    reranked_contexts = rerank_by_relevance(query, top_contexts_for_rerank)
    top_contexts = reranked_contexts[:8]

    answer = llm_fn(query, top_contexts)

    hit_ids = [hit.payload.get('‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', None) for hit in results]

    return answer, hit_ids

def agent(query, collection, top_k=40):
    answer, first_result_ids = process_search_and_answer(query, collection, top_k)

    if answer and "‡πÑ‡∏°‡πà‡∏û‡∏ö" not in answer.lower() and len(answer) >= 20:
        return answer

    logging.info("üîÅ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 2 ...")

    answer, _ = process_search_and_answer(query, collection, top_k, exclude_ids=first_result_ids)

    if not answer or "‡πÑ‡∏°‡πà‡∏û‡∏ö" in answer.lower() or len(answer) < 20:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô (‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ 2 ‡∏£‡∏≠‡∏ö)"

    return answer

@app.post("/webhook")
async def n8n_webhook(request: Request):
    data = await request.json()
    user_message = data.get("text", "").strip()

    logging.info(f"üì© Message from n8n webhook: {user_message}")

    if user_message.lower() == "/start":
        welcome_message = (
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏ú‡∏±‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ\n"
            "‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏≤‡∏ö"
        )
        return JSONResponse({"answer": welcome_message})

    answer = agent(user_message, QDRANT_COLLECTION)
    return JSONResponse({"answer": answer})
